# Demo of How to Install and Use the Paddle Inference Library Used on KUNLUN XPU Chips 

There are mainly three high-performance inference libraries which are used on the KUNLUN XPU chips and applicable to different senarios of the cloud, edge, or end. 

| Chinese Name               | English Name         | Application                     | Supported Language | Installation Method |
| ------------------ | ---------------- | ---------------------------- | ---------------------------- |---------------------------- |
| 飞桨原生推理库     | Paddle Inference | Inference on the high-performance server or the cloud    | Python、C++ | Download the whl package of the Python versionwhl or use the source-code compilation, namely the source-code compilation in C++ |
| 飞桨服务化推理框架 | Paddle Serving   | Senior functions like automatic serving, and model management | Python、C++ | Source-code compilation |
| 飞桨轻量化推理引擎 | Paddle Lite      | inference on mobile devices, IoT, and so on             | Python、C++ | Source-code compilation |

If you want to know how to install and use Paddle Inference 2.2, please [click](https://paddleinference.paddlepaddle.org.cn/demo_tutorial/paddle_xpu_infer_cn.html).

If you want to know how to install and use Paddle Serving 0.8.3, please [click](https://github.com/PaddlePaddle/Serving/blob/v0.8.3/doc/Run_On_XPU_CN.md).

If you want to know how to install and use Paddle Lite 2.10, please [click](https://paddlelite.paddlepaddle.org.cn/demo_guides/baidu_xpu.html).
